{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'Climate ID', 'max_temp_deg_c', 'min_temp_deg_c',\n",
      "       'mean_temp_deg_c', 'heat_deg_days_deg_c', 'cool_deg_days_deg_c',\n",
      "       'rain_mm', 'snow_cm', 'total_prcp_mm', 'snow_on_grnd_cm',\n",
      "       'dir_max_gust_10s_deg', 'spd_max_gust_km_h', 'dew_point_C',\n",
      "       'rel_hum_percent'],\n",
      "      dtype='object')\n",
      "None\n",
      "<Day>\n",
      "2011-01-01 00:00:00 2021-12-31 00:00:00\n",
      "Index(['date', 'Climate ID', 'max_temp_deg_c', 'min_temp_deg_c',\n",
      "       'mean_temp_deg_c', 'heat_deg_days_deg_c', 'cool_deg_days_deg_c',\n",
      "       'rain_mm', 'snow_cm', 'total_prcp_mm', 'snow_on_grnd_cm',\n",
      "       'dir_max_gust_10s_deg', 'spd_max_gust_km_h', 'dew_point_deg_c',\n",
      "       'rel_hum_percent', 'stage_m', 'q_m3_s'],\n",
      "      dtype='object')\n",
      "None\n",
      "<Day>\n",
      "            Climate ID  max_temp_deg_c  min_temp_deg_c  mean_temp_deg_c  \\\n",
      "date                                                                      \n",
      "2011-01-01     8202000             6.3             0.1              3.2   \n",
      "2011-01-02     8202000             1.9            -1.2              0.4   \n",
      "2011-01-03     8202000             1.6            -4.2             -1.3   \n",
      "2011-01-04     8202000            -1.4            -6.0             -3.7   \n",
      "2011-01-05     8202000            -1.1            -6.0             -3.6   \n",
      "...                ...             ...             ...              ...   \n",
      "2021-12-27     8202000             1.5            -6.9             -2.7   \n",
      "2021-12-28     8202000             2.5            -7.2             -2.4   \n",
      "2021-12-29     8202000             1.8            -1.5              0.2   \n",
      "2021-12-30     8202000             2.2            -1.5              0.4   \n",
      "2021-12-31     8202000             1.8             0.0              0.9   \n",
      "\n",
      "            heat_deg_days_deg_c  cool_deg_days_deg_c  rain_mm  snow_cm  \\\n",
      "date                                                                     \n",
      "2011-01-01                 14.8                  0.0      0.0      0.0   \n",
      "2011-01-02                 17.6                  0.0      1.8      0.0   \n",
      "2011-01-03                 19.3                  0.0      4.4      6.8   \n",
      "2011-01-04                 21.7                  0.0      0.0      0.0   \n",
      "2011-01-05                 21.6                  0.0      0.0      0.0   \n",
      "...                         ...                  ...      ...      ...   \n",
      "2021-12-27                 20.7                  0.0      0.0      0.0   \n",
      "2021-12-28                 20.4                  0.0      0.0      4.2   \n",
      "2021-12-29                 17.8                  0.0      0.0      0.4   \n",
      "2021-12-30                 17.6                  0.0      0.0      0.0   \n",
      "2021-12-31                 17.1                  0.0      1.6      1.4   \n",
      "\n",
      "            total_prcp_mm  snow_on_grnd_cm  dir_max_gust_10s_deg  \\\n",
      "date                                                               \n",
      "2011-01-01            0.0             14.0                   NaN   \n",
      "2011-01-02            1.8              8.0                   NaN   \n",
      "2011-01-03           11.2             10.0                  28.0   \n",
      "2011-01-04            0.0             11.0                  28.0   \n",
      "2011-01-05            0.0             10.0                  26.0   \n",
      "...                   ...              ...                   ...   \n",
      "2021-12-27            0.0              NaN                  32.0   \n",
      "2021-12-28            4.2              1.0                   NaN   \n",
      "2021-12-29            0.4              4.0                   NaN   \n",
      "2021-12-30            0.0              2.0                   NaN   \n",
      "2021-12-31            3.0              1.0                   NaN   \n",
      "\n",
      "           spd_max_gust_km_h  dew_point_deg_c  rel_hum_percent  stage_m  \\\n",
      "date                                                                      \n",
      "2011-01-01               <31         2.333333        87.875000    13.00   \n",
      "2011-01-02               <31         0.050000        96.166667    12.00   \n",
      "2011-01-03                57        -2.150000        88.208333    12.60   \n",
      "2011-01-04                54        -8.120833        70.708333    12.40   \n",
      "2011-01-05                46        -9.475000        63.000000    10.90   \n",
      "...                      ...              ...              ...      ...   \n",
      "2021-12-27              35.0        -3.250000        78.708333    10.60   \n",
      "2021-12-28               NaN        -4.191667        88.208333     9.73   \n",
      "2021-12-29               NaN        -0.604167        95.916667     9.08   \n",
      "2021-12-30               NaN        -1.041667        90.333333     8.36   \n",
      "2021-12-31               NaN         0.779167       100.000000     7.76   \n",
      "\n",
      "            q_m3_s  \n",
      "date                \n",
      "2011-01-01   1.102  \n",
      "2011-01-02   1.080  \n",
      "2011-01-03   1.092  \n",
      "2011-01-04   1.088  \n",
      "2011-01-05   1.067  \n",
      "...            ...  \n",
      "2021-12-27   1.029  \n",
      "2021-12-28   1.010  \n",
      "2021-12-29   0.996  \n",
      "2021-12-30   0.978  \n",
      "2021-12-31   0.963  \n",
      "\n",
      "[4018 rows x 16 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daryl\\AppData\\Local\\Temp\\ipykernel_6644\\236312825.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q_filt_stage.rename(columns={'Value': 'stage_m'}, inplace=True)\n",
      "C:\\Users\\daryl\\AppData\\Local\\Temp\\ipykernel_6644\\236312825.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q_filt_stage.drop(columns=['PARAM'], inplace=True)\n",
      "C:\\Users\\daryl\\AppData\\Local\\Temp\\ipykernel_6644\\236312825.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q_filt_q.rename(columns={'Value': 'q_m3_s'}, inplace=True)\n",
      "C:\\Users\\daryl\\AppData\\Local\\Temp\\ipykernel_6644\\236312825.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q_filt_q.drop(columns=['PARAM'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "dir = (r'C:\\Users\\daryl\\OneDrive\\Documents\\GDAA3000\\ProjectDischarge\\RdrsSample'\n",
    "       r'\\LstmDatasets\\NsWeatherData\\01ED007')\n",
    "q_dir = (r'C:\\Users\\daryl\\OneDrive\\Documents\\GDAA3000\\ProjectDischarge'\n",
    "         r'\\RdrsSample\\LstmDatasets\\basin_q')\n",
    "\n",
    "filenames = os.listdir(dir)\n",
    "q_files = os.listdir(q_dir)\n",
    "\n",
    "for file in filenames:\n",
    "    basin_id = file.split('_')[0]\n",
    "    if file.endswith('01ED007_weather_hourly-daily_joined_2011-2021_daily.csv'):\n",
    "        stn = os.path.join(dir, file)\n",
    "        df_stn = pd.read_csv(stn)\n",
    "        # Convert 'date' column to a datetime object\n",
    "        df_stn['date'] = pd.to_datetime(df_stn['date'])\n",
    "        df_stn['date'] = df_stn['date'].dt.strftime('%Y/%m/%d')\n",
    "        df_stn['date'] = pd.to_datetime(df_stn['date'])\n",
    "        # print(df_stn.head())\n",
    "        df_stn = df_stn.rename(columns={'max_tem_deg_c': 'max_temp_deg_c'})\n",
    "        print(df_stn.columns)\n",
    "\n",
    "\n",
    "        for q in q_files:\n",
    "            if q.endswith('01ED007_daily_20240507T2314.csv'):\n",
    "                q_csv = os.path.join(q_dir, q)\n",
    "                q_basin_id = q.split('_')[0]\n",
    "                if basin_id == q_basin_id:\n",
    "                    df_q = pd.read_csv(q_csv, parse_dates=['Date'], skiprows=1)\n",
    "                    # Rename the 'Date' column to 'date' in df_q.\n",
    "                    df_q.rename(columns={'Date': 'date'}, inplace=True)\n",
    "                    df_q['date'] = df_q['date'].dt.strftime('%Y/%m/%d')\n",
    "                    # Convert 'date' column to a datetime object\n",
    "                    df_q['date'] = pd.to_datetime(df_q['date'])\n",
    "                    # Make 'date' the index of df_q.\n",
    "                    # df_q.set_index('date', inplace=True)\n",
    "                    # print(df_q)\n",
    "                    ##### Filter to desired date range #############################\n",
    "                    # Find the earliest and latest dates in df_stn and use them to filter .\n",
    "                    start_date = df_stn['date'].min()\n",
    "                    end_date = df_stn['date'].max()\n",
    "                    \n",
    "                    # Filter df_q based on the date range\n",
    "                    df_q = df_q[(df_q['date'] >= start_date) & (df_q['date'] <= end_date)]\n",
    "                    ##### Reorganize the data ######################################\n",
    "                    df_q_filt_stage = df_q[df_q['PARAM'] == 1]\n",
    "                    # Filter df_q to only include rows where 'PARAM'== 2 (discharge rate).\n",
    "                    df_q_filt_q = df_q[df_q['PARAM'] == 2]\n",
    "                    # For df_q_filt_stage, rename the 'Value' column to 'stage_m' and drop the 'PARAM' column.\n",
    "                    df_q_filt_stage.rename(columns={'Value': 'stage_m'}, inplace=True)\n",
    "                    df_q_filt_stage.drop(columns=['PARAM'], inplace=True)\n",
    "                    # For df_q_filt_q, rename the 'Value' column to 'q_m3_s' and drop the 'PARAM' column.\n",
    "                    df_q_filt_q.rename(columns={'Value': 'q_m3_s'}, inplace=True)\n",
    "                    df_q_filt_q.drop(columns=['PARAM'], inplace=True)\n",
    "                    # print(df_q_filt_q)\n",
    "                    # Merge df_q_filt_stage and df_q_filt_q on 'date' using an inner join.\n",
    "                    df_q_joined = pd.merge(df_q_filt_stage, df_q_filt_q, on='date', how='inner')\n",
    "                    # Interpolate missing values in  for columns 'q_m3_s'.\n",
    "                    df_q_joined['q_m3_s'] = df_q_joined['q_m3_s'].interpolate()\n",
    "                    df_q_joined['stage_m'] = df_q_joined['stage_m'].interpolate()\n",
    "                    # Drop unnecessary columns from df_daymet. Warning: there is a space in the ' ID' \n",
    "                    # and ' ID_y' column names.\n",
    "                    df_q_joined.drop(columns=[ ' ID_x', ' ID_y', 'SYM_x', 'SYM_y'], inplace=True)\n",
    "                    # print(df_q_joined.columns)\n",
    "                    # print(df_q_joined)\n",
    "\n",
    "                    # # Set the date as the index\n",
    "                    df_q_joined.set_index('date', inplace=True)\n",
    "\n",
    "                    # Check if the index is regularly spaced\n",
    "                    print(df_q_joined.index.freq)\n",
    "\n",
    "                    # Make the index regularly spaced (e.g., daily)\n",
    "                    df_q_joined = df_q_joined.asfreq('D')\n",
    "\n",
    "                    # Check if the index is regularly spaced\n",
    "                    print(df_q_joined.index.freq)\n",
    "                    # Print the first and last dates of the index\n",
    "                    print(df_q_joined.index[0], df_q_joined.index[-1])\n",
    "                    # print(df_q_joined.head())\n",
    "                    # print(df_stn.head())\n",
    "                    # Merge df_daymet and df_q_joined on 'date' using an inner join.\n",
    "                    df_q_joined = pd.merge(df_stn, df_q_joined, on='date', how='inner')\n",
    "                    # Change date format to 'dd/mm/yyyy'.\n",
    "                    \n",
    "                    df_q_joined = df_q_joined.rename(columns={'dew_point_C': 'dew_point_deg_c'})\n",
    "                    df_q_joined.replace('', np.nan, inplace=True)\n",
    "                    # print(df_q_joined.columns)\n",
    "                    output_dir = (r'C:\\Users\\daryl\\OneDrive\\Documents\\GDAA3000'\n",
    "                                  r'\\ProjectDischarge\\RdrsSample\\LstmDatasets\\JoinedQWeatherStn')\n",
    "                    # df_q_joined_dropped = df_q_joined.drop(columns=['Id'])\n",
    "                \n",
    "                    # Get a list of all the columns\n",
    "                    cols = list(df_q_joined.columns)\n",
    "                    # print(cols)\n",
    "                    # Remove 'date' from the list\n",
    "                    cols.remove('date')\n",
    "                    # Reorder the columns to make 'date' the first column\n",
    "                    df_q_joined_dropped = df_q_joined[['date'] + cols]\n",
    "                    print(df_q_joined_dropped.columns)\n",
    "                    # Interpolate missing values in df_q_joined_dropped\n",
    "                    df_q_joined_dropped['max_temp_deg_c'] = df_q_joined_dropped['max_temp_deg_c'].interpolate()\n",
    "                    df_q_joined_dropped['min_temp_deg_c'] = df_q_joined_dropped['min_temp_deg_c'].interpolate()\n",
    "                    df_q_joined_dropped['mean_temp_deg_c'] = df_q_joined_dropped['mean_temp_deg_c'].interpolate()\n",
    "                    df_q_joined_dropped['q_m3_s'] = df_q_joined_dropped['q_m3_s'].interpolate()\n",
    "                    df_q_joined_dropped['dew_point_deg_c'] = df_q_joined_dropped['dew_point_deg_c'].interpolate()\n",
    "                    df_q_joined_dropped['rel_hum_percent'] = df_q_joined_dropped['rel_hum_percent'].interpolate()\n",
    "                    # Set the date as the index\n",
    "                    df_q_joined_dropped.set_index('date', inplace=True)\n",
    "\n",
    "                    # Check if the index is regularly spaced\n",
    "                    print(df_q_joined_dropped.index.freq)\n",
    "\n",
    "                    # Make the index regularly spaced (e.g., daily)\n",
    "                    df_q_joined_dropped = df_q_joined_dropped.asfreq('D')\n",
    "\n",
    "                    # Check if the index is regularly spaced\n",
    "                    print(df_q_joined_dropped.index.freq)\n",
    "                    # Save the joined dataframe as a csv file in the output_dir.\n",
    "                    df_q_joined_dropped.to_csv(os.path.join(output_dir, f'{q_basin_id}_stn_2011-2021_q_new.csv'))\n",
    "                    print(df_q_joined_dropped)\n",
    "\n",
    "                    \n",
    "\n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    #### Check for missing values ################################\n",
    "                    Identify groups of consecutive missing values\n",
    "                    df_q['group'] = df_q['Value'].isna().ne(df_q['Value'].shift().isna()).cumsum()\n",
    "\n",
    "                    # Count the number of missing values in each group\n",
    "                    counts = df_q.groupby('group')['Value'].apply(lambda x: x.isna().sum())\n",
    "\n",
    "                    # Calculate the total number of rows for groups with more than 10 consecutive missing values\n",
    "                    total_rows = counts[counts > 10].sum()\n",
    "\n",
    "                    # Calculate 10% of the total original number of rows in df_q\n",
    "                    ten_percent = df_q.shape[0] * 0.1\n",
    "\n",
    "                    # If the total number of rows for groups with more than 10 consecutive missing values is greater than 10% of the total original number of rows in df_q\n",
    "                    if total_rows > ten_percent:\n",
    "                        # Calculate the percentage of the df_q that is groups with more than 10 consecutive missing values\n",
    "                        percent = (total_rows / df_q.shape[0]) * 100\n",
    "                        # print(f\"{percent}% of {q_basin_id} is groups with more than 10 consecutive missing values.\")\n",
    "                        break\n",
    "                    elif total_rows <= ten_percent:\n",
    "                        # impute missing values in df_q\n",
    "                        df_q['Value'] = df_q['Value'].interpolate()\n",
    "\n",
    "                    # Print the maximum number of consecutive missing values in the 'Value' column\n",
    "                    print(f\"Max number of consecutive missing values in {q_basin_id}: {counts.max()}\")\n",
    "                    # Print the percent of missing values in the 'Value' column.\n",
    "                    print(f\"Percent of missing values in {q_basin_id}: {df_q['Value'].isna().mean() * 100}\")\n",
    "                    print()\n",
    "                    Convert missing values to NaN in df_q_joined"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
