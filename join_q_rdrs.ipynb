{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daryl\\AppData\\Local\\Temp\\ipykernel_18544\\1923203972.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q_filt_stage.rename(columns={'Value': 'stage_m'}, inplace=True)\n",
      "C:\\Users\\daryl\\AppData\\Local\\Temp\\ipykernel_18544\\1923203972.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q_filt_stage.drop(columns=['PARAM'], inplace=True)\n",
      "C:\\Users\\daryl\\AppData\\Local\\Temp\\ipykernel_18544\\1923203972.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q_filt_q.rename(columns={'Value': 'q_m3_s'}, inplace=True)\n",
      "C:\\Users\\daryl\\AppData\\Local\\Temp\\ipykernel_18544\\1923203972.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q_filt_q.drop(columns=['PARAM'], inplace=True)\n",
      "C:\\Users\\daryl\\AppData\\Local\\Temp\\ipykernel_18544\\1923203972.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q_filt_stage.rename(columns={'Value': 'stage_m'}, inplace=True)\n",
      "C:\\Users\\daryl\\AppData\\Local\\Temp\\ipykernel_18544\\1923203972.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q_filt_stage.drop(columns=['PARAM'], inplace=True)\n",
      "C:\\Users\\daryl\\AppData\\Local\\Temp\\ipykernel_18544\\1923203972.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q_filt_q.rename(columns={'Value': 'q_m3_s'}, inplace=True)\n",
      "C:\\Users\\daryl\\AppData\\Local\\Temp\\ipykernel_18544\\1923203972.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q_filt_q.drop(columns=['PARAM'], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of consecutive missing values in 01DR001: 42\n",
      "Percent of missing values in 01DR001: 0.0\n",
      "\n",
      "Index(['date', 'basin', 'A_PRO_SFC', 'P_HR_1-5m', 'P_HU_1-5m', 'P_TD_1-5m',\n",
      "       'P_TT_1-5m', 'P_UUC_10m', 'P_VVC_10m', 'P_FB_SFC', 'stage_m', 'q_m3_s'],\n",
      "      dtype='object')\n",
      "         date    basin  A_PRO_SFC  P_HR_1-5m  P_HU_1-5m  P_TD_1-5m  P_TT_1-5m  \\\n",
      "0  01/01/2011  01DR001   0.000486   0.907476   0.003406  -1.510888  -0.167569   \n",
      "1  02/01/2011  01DR001   0.000244   0.920298   0.003576  -0.932975   0.283169   \n",
      "2  03/01/2011  01DR001   0.019123   0.886419   0.003053  -4.020798  -2.385277   \n",
      "3  04/01/2011  01DR001   0.000175   0.754222   0.001673 -10.928314  -7.213029   \n",
      "4  05/01/2011  01DR001   0.000000   0.722910   0.001447 -12.929564  -8.620917   \n",
      "\n",
      "   P_UUC_10m  P_VVC_10m   P_FB_SFC  stage_m  q_m3_s  \n",
      "0   5.940687  -4.905020  41.276009     6.53   1.192  \n",
      "1   2.051195  -2.407153  78.966560     5.65   1.151  \n",
      "2   5.345839   1.127073  13.419610     6.65   1.189  \n",
      "3   5.219585   5.020722  60.717484     6.98   1.215  \n",
      "4   1.327541   4.860843  84.048584     3.94   1.111  \n",
      "Max number of consecutive missing values in 01ED007: 19\n",
      "Percent of missing values in 01ED007: 0.0\n",
      "\n",
      "Index(['date', 'basin', 'A_PRO_SFC', 'P_HR_1-5m', 'P_HU_1-5m', 'P_TD_1-5m',\n",
      "       'P_TT_1-5m', 'P_UUC_10m', 'P_VVC_10m', 'P_FB_SFC', 'stage_m', 'q_m3_s'],\n",
      "      dtype='object')\n",
      "         date    basin  A_PRO_SFC  P_HR_1-5m  P_HU_1-5m  P_TD_1-5m  P_TT_1-5m  \\\n",
      "0  01/01/2011  01ED007   0.000003   0.928137   0.003557  -1.265154  -0.207094   \n",
      "1  02/01/2011  01ED007   0.014466   0.962620   0.003888   0.104665   0.654497   \n",
      "2  03/01/2011  01ED007   0.004528   0.747964   0.002308  -7.253989  -3.395887   \n",
      "3  04/01/2011  01ED007   0.000091   0.743326   0.001952  -9.040756  -5.107268   \n",
      "4  05/01/2011  01ED007   0.000240   0.681840   0.001689 -10.927714  -5.797690   \n",
      "\n",
      "   P_UUC_10m  P_VVC_10m   P_FB_SFC  stage_m  q_m3_s  \n",
      "0   0.039327  -1.862934  79.118416     13.0   1.102  \n",
      "1  -1.564966  -0.487981  23.596991     12.0   1.080  \n",
      "2   9.585446  -2.054624  48.331970     12.6   1.092  \n",
      "3   6.473141   2.557409  64.464806     12.4   1.088  \n",
      "4   5.295787   2.758596  88.471947     10.9   1.067  \n",
      "Max number of consecutive missing values in 01FB001: 49\n",
      "Percent of missing values in 01FB001: 0.0\n",
      "\n",
      "Index(['date', 'basin', 'A_PRO_SFC', 'P_HR_1-5m', 'P_HU_1-5m', 'P_TD_1-5m',\n",
      "       'P_TT_1-5m', 'P_UUC_10m', 'P_VVC_10m', 'P_FB_SFC', 'stage_m', 'q_m3_s'],\n",
      "      dtype='object')\n",
      "         date    basin  A_PRO_SFC  P_HR_1-5m  P_HU_1-5m  P_TD_1-5m  P_TT_1-5m  \\\n",
      "0  01/01/2011  01FB001   0.000232   0.884276   0.003182  -2.797919  -1.103277   \n",
      "1  02/01/2011  01FB001   0.000064   0.950140   0.003686  -0.907769  -0.184114   \n",
      "2  03/01/2011  01FB001   0.017278   0.934515   0.003332  -2.591340  -1.675172   \n",
      "3  04/01/2011  01FB001   0.008502   0.821724   0.002021  -8.946668  -6.407900   \n",
      "4  05/01/2011  01FB001   0.000449   0.773536   0.001744 -10.852074  -7.519195   \n",
      "\n",
      "   P_UUC_10m  P_VVC_10m   P_FB_SFC  stage_m  q_m3_s  \n",
      "0   7.298950  -6.543032  60.683731     23.0   0.854  \n",
      "1   1.037218  -2.044527  62.469193     21.9   0.832  \n",
      "2  -0.000033   2.467321  30.739218     21.5   0.824  \n",
      "3   8.947338   7.063440  40.064148     21.1   0.817  \n",
      "4   1.637119   5.901152  72.565765     20.4   0.801  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daryl\\AppData\\Local\\Temp\\ipykernel_18544\\1923203972.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q_filt_stage.rename(columns={'Value': 'stage_m'}, inplace=True)\n",
      "C:\\Users\\daryl\\AppData\\Local\\Temp\\ipykernel_18544\\1923203972.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q_filt_stage.drop(columns=['PARAM'], inplace=True)\n",
      "C:\\Users\\daryl\\AppData\\Local\\Temp\\ipykernel_18544\\1923203972.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q_filt_q.rename(columns={'Value': 'q_m3_s'}, inplace=True)\n",
      "C:\\Users\\daryl\\AppData\\Local\\Temp\\ipykernel_18544\\1923203972.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q_filt_q.drop(columns=['PARAM'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "rdrs_dir = (r'C:\\Users\\daryl\\OneDrive\\Documents\\GDAA3000\\ProjectDischarge'\n",
    "            r'\\RdrsSample\\LstmDatasets\\10YrNhRun\\Rdrs10yrCsv')\n",
    "q_dir = (r'C:\\Users\\daryl\\OneDrive\\Documents\\GDAA3000\\ProjectDischarge'\n",
    "         r'\\RdrsSample\\LstmDatasets\\basin_q')\n",
    "\n",
    "rdrs_files = os.listdir(rdrs_dir)\n",
    "q_files = os.listdir(q_dir)\n",
    "\n",
    "for file in rdrs_files:\n",
    "    rdrs_basin_id = file.split('_')[0]\n",
    "    if file.endswith('.csv'):\n",
    "        rdrs = os.path.join(rdrs_dir, file)\n",
    "        df_rdrs = pd.read_csv(rdrs, parse_dates=['date'])\n",
    "\n",
    "        for q in q_files:\n",
    "            if q.endswith('.csv'):\n",
    "                q_basin_id = q.split('_')[0]\n",
    "                q = os.path.join(q_dir, q)\n",
    "                if rdrs_basin_id == q_basin_id:\n",
    "                    df_q = pd.read_csv(q, parse_dates=['Date'], skiprows=1)\n",
    "                    # Rename the 'Date' column to 'date' in df_q.\n",
    "                    df_q.rename(columns={'Date': 'date'}, inplace=True)\n",
    "\n",
    "                    ##### Filter to desired date range #############################\n",
    "                    # Find the earliest and latest dates in df_rdrs and use them to filter .\n",
    "                    start_date = df_rdrs['date'].min()\n",
    "                    end_date = df_rdrs['date'].max()\n",
    "                    # Filter df_q based on the date range\n",
    "                    df_q = df_q[(df_q['date'] >= start_date) & (df_q['date'] <= end_date)]\n",
    "                    ##### Reorganize the data ######################################\n",
    "                    df_q_filt_stage = df_q[df_q['PARAM'] == 1]\n",
    "                    # Filter df_q to only include rows where 'PARAM'== 2 (discharge rate).\n",
    "                    df_q_filt_q = df_q[df_q['PARAM'] == 2]\n",
    "                    # For df_q_filt_stage, rename the 'Value' column to 'stage_m' and drop the 'PARAM' column.\n",
    "                    df_q_filt_stage.rename(columns={'Value': 'stage_m'}, inplace=True)\n",
    "                    df_q_filt_stage.drop(columns=['PARAM'], inplace=True)\n",
    "                    # For df_q_filt_q, rename the 'Value' column to 'q_m3_s' and drop the 'PARAM' column.\n",
    "                    df_q_filt_q.rename(columns={'Value': 'q_m3_s'}, inplace=True)\n",
    "                    df_q_filt_q.drop(columns=['PARAM'], inplace=True)\n",
    "                    # Merge df_q_filt_stage and df_q_filt_q on 'date' using an inner join.\n",
    "                    df_q_joined = pd.merge(df_q_filt_stage, df_q_filt_q, on='date', how='inner')\n",
    "                    # Interpolate missing values in  for columns 'q_m3_s'.\n",
    "                    df_q_joined['q_m3_s'] = df_q_joined['q_m3_s'].interpolate()\n",
    "                    df_q_joined['stage_m'] = df_q_joined['stage_m'].interpolate()\n",
    "                    # Drop unnecessary columns from df_rdrs. Warning: there is a space in the ' ID' \n",
    "                    # and ' ID_y' column names.\n",
    "                    df_q_joined.drop(columns=[' ID_y', 'SYM_x', 'SYM_y'], inplace=True)\n",
    "                    df_q_joined.rename(columns={' ID_x': 'Id'}, inplace=True)\n",
    "                    # Merge df_rdrs and df_q_joined on 'date' using an inner join.\n",
    "                    df_q_joined = pd.merge(df_rdrs, df_q_joined, on='date', how='inner')\n",
    "                    # Change date format to 'dd/mm/yyyy'.\n",
    "                    df_q_joined['date'] = pd.to_datetime(df_q_joined['date']).dt.strftime('%d/%m/%Y')\n",
    "\n",
    "                    ##### Check for missing values ################################\n",
    "                    # Identify groups of consecutive missing values\n",
    "                    df_q['group'] = df_q['Value'].isna().ne(df_q['Value'].shift().isna()).cumsum()\n",
    "\n",
    "                    # Count the number of missing values in each group\n",
    "                    counts = df_q.groupby('group')['Value'].apply(lambda x: x.isna().sum())\n",
    "\n",
    "                    # Calculate the total number of rows for groups with more than 10 consecutive missing values\n",
    "                    total_rows = counts[counts > 10].sum()\n",
    "\n",
    "                    # Calculate 10% of the total original number of rows in df_q\n",
    "                    ten_percent = df_q.shape[0] * 0.1\n",
    "\n",
    "                    # If the total number of rows for groups with more than 10 consecutive missing values is greater than 10% of the total original number of rows in df_q\n",
    "                    if total_rows > ten_percent:\n",
    "                        # Calculate the percentage of the df_q that is groups with more than 10 consecutive missing values\n",
    "                        percent = (total_rows / df_q.shape[0]) * 100\n",
    "                        # print(f\"{percent}% of {q_basin_id} is groups with more than 10 consecutive missing values.\")\n",
    "                        break\n",
    "                    elif total_rows <= ten_percent:\n",
    "                        # impute missing values in df_q\n",
    "                        df_q['Value'] = df_q['Value'].interpolate()\n",
    "\n",
    "                    # Print the maximum number of consecutive missing values in the 'Value' column\n",
    "                    print(f\"Max number of consecutive missing values in {q_basin_id}: {counts.max()}\")\n",
    "                    # Print the percent of missing values in the 'Value' column.\n",
    "                    print(f\"Percent of missing values in {q_basin_id}: {df_q['Value'].isna().mean() * 100}\")\n",
    "                    print()\n",
    "\n",
    "\n",
    "                    output_dir = (r'C:\\Users\\daryl\\OneDrive\\Documents\\GDAA3000'\n",
    "                                  r'\\ProjectDischarge\\RdrsSample\\LstmDatasets\\10YrNhRun\\Rdrs10yrsJoinedQCsv')\n",
    "                    df_q_joined_dropped = df_q_joined.drop(columns=['Id'])\n",
    "                    df_q_joined_dropped = df_q_joined_dropped.rename(columns={'RDRS_v2.1_A_PR0_SFC': 'A_PRO_SFC',\n",
    "                                                                              'RDRS_v2.1_P_HR_1.5m': 'P_HR_1-5m',\n",
    "                                                                              'RDRS_v2.1_P_HU_1.5m': 'P_HU_1-5m',\n",
    "                                                                              'RDRS_v2.1_P_TD_1.5m': 'P_TD_1-5m',\n",
    "                                                                              'RDRS_v2.1_P_TT_1.5m': 'P_TT_1-5m',\n",
    "                                                                              'RDRS_v2.1_P_UUC_10m': 'P_UUC_10m',\n",
    "                                                                              'RDRS_v2.1_P_VVC_10m': 'P_VVC_10m',\n",
    "                                                                              'RDRS_v2.1_P_FB_SFC': 'P_FB_SFC',})\n",
    "                    # Save the joined dataframe as a csv file in the output_dir.\n",
    "                    df_q_joined_dropped.to_csv(os.path.join(output_dir, f'{q_basin_id}.csv'), index=False)\n",
    "                    print(df_q_joined_dropped.columns)\n",
    "                    print(df_q_joined_dropped.head())                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
