{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in 01FJ002: 91\n",
      "Number of rows in 01FJ002: 8010\n",
      "Percent of missing values in 01FJ002: 1.136079900124844\n",
      "Number of groups with 5 or more missing values in 01FJ002: 5\n",
      "Max number of consecutive missing values in 01FJ002: 30\n",
      "Percent of missing values in 01FJ002: 0.0\n",
      "\n",
      "            date    dayl_s  prcp_mm  srad_W_m2  swe_kg_m2  tmax_deg_c  \\\n",
      "0     2011/01/01  30634.02     0.00      99.84      12.18        3.94   \n",
      "1     2011/01/02  30687.14     0.00     115.73       9.40        5.13   \n",
      "2     2011/01/03  30744.59    11.33      84.84       7.39        3.10   \n",
      "3     2011/01/04  30806.31     0.00     115.66       7.34       -1.57   \n",
      "4     2011/01/05  30872.26     0.00     176.45       7.34       -1.59   \n",
      "...          ...       ...      ...        ...        ...         ...   \n",
      "3997  2021/12/14  30449.68     5.32     100.25       0.00        4.05   \n",
      "3998  2021/12/15  30421.01     0.82      50.68       0.00       -1.52   \n",
      "3999  2021/12/16  30396.87     2.09      97.25       0.00        4.11   \n",
      "4000  2021/12/17  30377.31     1.80     129.91       0.00        9.68   \n",
      "4001  2021/12/18  30362.32     0.00      78.49       0.00        2.30   \n",
      "\n",
      "      tmin_deg_c   vp_Pa  stage_m  q_m3_s  \n",
      "0           0.14  617.09    0.645   1.061  \n",
      "1           0.62  638.89    0.498   1.033  \n",
      "2          -1.22  558.71    0.408   1.020  \n",
      "3          -5.88  394.00    0.304   1.021  \n",
      "4          -9.23  303.83    0.199   0.995  \n",
      "...          ...     ...      ...     ...  \n",
      "3997       -3.80  461.13    1.780   1.153  \n",
      "3998       -3.97  455.17    1.090   1.054  \n",
      "3999       -3.38  476.05    0.758   0.995  \n",
      "4000        1.82  696.23    0.776   0.998  \n",
      "4001       -1.73  538.07    0.735   0.993  \n",
      "\n",
      "[4002 rows x 10 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daryl\\AppData\\Local\\Temp\\ipykernel_17588\\1502946557.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q_filt_stage.rename(columns={'Value': 'stage_m'}, inplace=True)\n",
      "C:\\Users\\daryl\\AppData\\Local\\Temp\\ipykernel_17588\\1502946557.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q_filt_stage.drop(columns=['PARAM'], inplace=True)\n",
      "C:\\Users\\daryl\\AppData\\Local\\Temp\\ipykernel_17588\\1502946557.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q_filt_q.rename(columns={'Value': 'q_m3_s'}, inplace=True)\n",
      "C:\\Users\\daryl\\AppData\\Local\\Temp\\ipykernel_17588\\1502946557.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_q_filt_q.drop(columns=['PARAM'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "dir = (r'C:\\Users\\daryl\\OneDrive\\Documents\\GDAA3000\\ProjectDischarge'\n",
    "            r'\\RdrsSample\\LstmDatasets\\Daymet')\n",
    "q_dir = (r'C:\\Users\\daryl\\OneDrive\\Documents\\GDAA3000\\ProjectDischarge'\n",
    "         r'\\RdrsSample\\LstmDatasets\\basin_q')\n",
    "\n",
    "filenames = os.listdir(dir)\n",
    "q_files = os.listdir(q_dir)\n",
    "\n",
    "for file in filenames:\n",
    "    basin_id = file.split('_')[0]\n",
    "    if file.endswith('01FJ002_daymet_2011-2021.csv'):\n",
    "        daymet = os.path.join(dir, file)\n",
    "        df_daymet = pd.read_csv(daymet, skiprows=6)\n",
    "        # Convert the 'year' and 'yday' columns to a date\n",
    "        df_daymet['date'] = pd.to_datetime(df_daymet['year'].astype(str) + df_daymet['yday'].astype(str), format='%Y%j')\n",
    "        # Drop the 'year' and 'yday' columns\n",
    "        df_daymet.drop(columns=['year', 'yday'], inplace=True)\n",
    "        df_daymet['date'] = df_daymet['date'].dt.strftime('%Y/%m/%d')\n",
    "        for q in q_files:\n",
    "            if q.endswith('.csv'):\n",
    "                q_basin_id = q.split('_')[0]\n",
    "                q = os.path.join(q_dir, q)\n",
    "                if basin_id == q_basin_id:\n",
    "                    df_q = pd.read_csv(q, parse_dates=['Date'], skiprows=1)\n",
    "                    # Rename the 'Date' column to 'date' in df_q.\n",
    "                    df_q.rename(columns={'Date': 'date'}, inplace=True)\n",
    "                    df_q['date'] = df_q['date'].dt.strftime('%Y/%m/%d')\n",
    "                    \n",
    "                    ##### Filter to desired date range #############################\n",
    "                    # Find the earliest and latest dates in df_daymet and use them to filter .\n",
    "                    start_date = df_daymet['date'].min()\n",
    "                    end_date = df_daymet['date'].max()\n",
    "                    # Filter df_q based on the date range\n",
    "                    df_q = df_q[(df_q['date'] >= start_date) & (df_q['date'] <= end_date)]\n",
    "                    ##### Reorganize the data ######################################\n",
    "                    df_q_filt_stage = df_q[df_q['PARAM'] == 1]\n",
    "                    # Filter df_q to only include rows where 'PARAM'== 2 (discharge rate).\n",
    "                    df_q_filt_q = df_q[df_q['PARAM'] == 2]\n",
    "                    # For df_q_filt_stage, rename the 'Value' column to 'stage_m' and drop the 'PARAM' column.\n",
    "                    df_q_filt_stage.rename(columns={'Value': 'stage_m'}, inplace=True)\n",
    "                    df_q_filt_stage.drop(columns=['PARAM'], inplace=True)\n",
    "                    # For df_q_filt_q, rename the 'Value' column to 'q_m3_s' and drop the 'PARAM' column.\n",
    "                    df_q_filt_q.rename(columns={'Value': 'q_m3_s'}, inplace=True)\n",
    "                    df_q_filt_q.drop(columns=['PARAM'], inplace=True)\n",
    "                    # Merge df_q_filt_stage and df_q_filt_q on 'date' using an inner join.\n",
    "                    df_q_joined = pd.merge(df_q_filt_stage, df_q_filt_q, on='date', how='inner')\n",
    "                    # Interpolate missing values in  for columns 'q_m3_s'.\n",
    "                    df_q_joined['q_m3_s'] = df_q_joined['q_m3_s'].interpolate()\n",
    "                    df_q_joined['stage_m'] = df_q_joined['stage_m'].interpolate()\n",
    "                    # Drop unnecessary columns from df_daymet. Warning: there is a space in the ' ID' \n",
    "                    # and ' ID_y' column names.\n",
    "                    df_q_joined.drop(columns=[' ID_y', 'SYM_x', 'SYM_y'], inplace=True)\n",
    "                    df_q_joined.rename(columns={' ID_x': 'Id'}, inplace=True)\n",
    "                    # Merge df_daymet and df_q_joined on 'date' using an inner join.\n",
    "                    df_q_joined = pd.merge(df_daymet, df_q_joined, on='date', how='inner')\n",
    "                    # Change date format to 'dd/mm/yyyy'.\n",
    "\n",
    "                    ##### Check for missing values ################################\n",
    "                    # Count number of missing values in the 'Value' column.\n",
    "                    print(f\"Number of missing values in {q_basin_id}: {df_q['Value'].isna().sum()}\")\n",
    "                    # Count the number of rows in df_q.\n",
    "                    print(f\"Number of rows in {q_basin_id}: {df_q.shape[0]}\")\n",
    "                    # Calculate the percentage of missing values in the 'Value' column.\n",
    "                    print(f\"Percent of missing values in {q_basin_id}: {df_q['Value'].isna().mean() * 100}\")\n",
    "                    \n",
    "                    # Identify groups of consecutive missing values\n",
    "                    df_q['group'] = df_q['Value'].isna().ne(df_q['Value'].shift().isna()).cumsum()\n",
    "\n",
    "                    # Count the number of missing values in each group\n",
    "                    counts = df_q.groupby('group')['Value'].apply(lambda x: x.isna().sum())\n",
    "                    # Count the number of groups with 5 or more missing values\n",
    "                    num_groups = (counts >= 5).sum()\n",
    "                    # Print the number of groups with 5 or more missing values\n",
    "                    print(f\"Number of groups with 5 or more missing values in {q_basin_id}: {num_groups}\")\n",
    "                    # impute missing values in df_q\n",
    "                    df_q['Value'] = df_q['Value'].interpolate()\n",
    "\n",
    "                    # Print the maximum number of consecutive missing values in the 'Value' column\n",
    "                    print(f\"Max number of consecutive missing values in {q_basin_id}: {counts.max()}\")\n",
    "                    # Print the percent of missing values in the 'Value' column.\n",
    "                    print(f\"Percent of missing values in {q_basin_id}: {df_q['Value'].isna().mean() * 100}\")\n",
    "                    print()\n",
    "\n",
    "\n",
    "                    output_dir = (r'C:\\Users\\daryl\\OneDrive\\Documents\\GDAA3000\\ProjectDischarge'\n",
    "                                  r'\\RdrsSample\\LstmDatasets\\DaymetJoinedQ')\n",
    "                    df_q_joined_dropped = df_q_joined.drop(columns=['Id'])\n",
    "                    df_q_joined_dropped = df_q_joined_dropped.rename(\n",
    "                        columns={'dayl (s)': 'dayl_s', 'prcp (mm/day)': 'prcp_mm',\n",
    "                                'srad (W/m^2)': 'srad_W_m2', 'swe (kg/m^2)': 'swe_kg_m2',\n",
    "                                'tmax (deg c)': 'tmax_deg_c', 'tmin (deg c)': 'tmin_deg_c',\n",
    "                                'vp (Pa)': 'vp_Pa'})\n",
    "                    # Get a list of all the columns\n",
    "                    cols = list(df_q_joined_dropped.columns)\n",
    "                    # Remove 'date' from the list\n",
    "                    cols.remove('date')\n",
    "                    # Reorder the columns to make 'date' the first column\n",
    "                    df_q_joined_dropped = df_q_joined_dropped[['date'] + cols]\n",
    "                    # Save the joined dataframe as a csv file in the output_dir.\n",
    "                    df_q_joined_dropped.to_csv(os.path.join(output_dir, f'{q_basin_id}_daymet_2011-2021yr_q_v2.csv'), index=False)\n",
    "                    print(df_q_joined_dropped)\n",
    "                                   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
