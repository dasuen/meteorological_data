{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob as gl\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Climate ID', 'date', 'temp_C', 'dew_point_C', 'rel_hum_percent'], dtype='object')\n",
      "   Climate ID                date  temp_C  dew_point_C  rel_hum_percent\n",
      "0     8202000 2011-01-01 00:00:00     3.0          0.9             86.0\n",
      "1     8202000 2011-01-01 01:00:00     3.8          1.5             85.0\n",
      "2     8202000 2011-01-01 02:00:00     4.0          2.2             88.0\n",
      "3     8202000 2011-01-01 03:00:00     3.8          2.0             88.0\n",
      "4     8202000 2011-01-01 04:00:00     3.6          2.0             89.0\n"
     ]
    }
   ],
   "source": [
    "# Specify the directory you want to search\n",
    "dir = (r'C:\\Users\\daryl\\OneDrive\\Documents\\GDAA3000\\ProjectDischarge'\n",
    "       r'\\RdrsSample\\LstmDatasets\\NsWeatherData\\01ED007\\Hourly')\n",
    "\n",
    "# Get a list of all CSV files in the directory\n",
    "csv_files = gl.glob(dir + '/*8202000*.csv')\n",
    "dfs = []\n",
    "\n",
    "for csv in csv_files:\n",
    "    try:\n",
    "        # Try to read the CSV file with 'utf-8' encoding\n",
    "        with open(csv, 'rb') as f:\n",
    "            content = f.read().decode('utf-8')\n",
    "        df = pd.read_csv(io.StringIO(content))\n",
    "    except UnicodeDecodeError:\n",
    "        # If there's a UnicodeDecodeError, try a different encoding\n",
    "        with open(csv, 'rb') as f:\n",
    "            content = f.read().decode('ISO-8859-1')\n",
    "        df = pd.read_csv(io.StringIO(content))\n",
    "    dfs.append(df)\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "df_all = df_all.drop(columns=['Longitude (x)', 'Latitude (y)', 'Station Name',\n",
    "                              'Year', 'Month', 'Day', 'Temp Flag', 'Dew Point Temp Flag',\n",
    "                              'Rel Hum Flag', 'Precip. Amount (mm)', 'Precip. Amount Flag',\n",
    "                              'Wind Dir Flag', 'Visibility (km)', 'Visibility Flag',\n",
    "                              'Stn Press Flag', 'Hmdx','Hmdx Flag', 'Wind Chill',\n",
    "                              'Wind Chill Flag', 'Weather', 'Wind Dir (10s deg)', \n",
    "                              'Wind Spd (km/h)', 'Wind Spd Flag', 'Stn Press (kPa)',\n",
    "                              'Time (LST)'] )\n",
    "\n",
    "# # Make Date/Time (LST) column the index\n",
    "# df_all['Date/Time (LST)'] = pd.to_datetime(df_all['Date/Time (LST)'])\n",
    "# # Convert Date/Time (LST) to datetime UTC\n",
    "# df_all['Date/Time (UTC)'] = df_all['Date/Time (LST)'].dt.tz_localize(\n",
    "#     'America/Toronto', nonexistent='shift_forward', ambiguous='NaT').dt.tz_convert('UTC')\n",
    "# df_all = df_all.drop(columns=['Date/Time (LST)'])\n",
    "\n",
    "df_all = df_all.rename(columns={'Date/Time (LST)': 'date', 'Temp (°C)': 'temp_C',\n",
    "                                'Dew Point Temp (°C)': 'dew_point_C',\n",
    "                                'Rel Hum (%)': 'rel_hum_percent', 'Date/Time (UTC)': 'date'})\n",
    "# Rename 'Date/Time (UTC)' to 'date'\n",
    "df_all = df_all.rename(columns={'Date/Time (UTC)': 'date'})\n",
    "# Convert 'date' column to datetime\n",
    "df_all['date'] = pd.to_datetime(df_all['date'])\n",
    "\n",
    "print(df_all.columns)\n",
    "print(df_all.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert hourly to daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<Day>\n",
      "2011-01-01 00:00:00 2021-12-31 00:00:00\n",
      "            dew_point_C  rel_hum_percent\n",
      "date                                    \n",
      "2011-01-01     2.333333        87.875000\n",
      "2011-01-02     0.050000        96.166667\n",
      "2011-01-03    -2.150000        88.208333\n",
      "2011-01-04    -8.120833        70.708333\n",
      "2011-01-05    -9.475000        63.000000\n"
     ]
    }
   ],
   "source": [
    "# Convert 'date' column to datetime object and assign dataframe to df_all.\n",
    "\n",
    "\n",
    "# Set 'date' column as index\n",
    "df_all.set_index('date', inplace=True)\n",
    "\n",
    "# Resample to daily frequency and calculate the mean\n",
    "df_daily = df_all[['dew_point_C', 'rel_hum_percent']].resample('D').mean()\n",
    "# Resample to daily frequency and calculate the mean\n",
    "df_daily = df_all[['dew_point_C', 'rel_hum_percent']].resample('D').mean()\n",
    "\n",
    "# df_daily.to_csv(file_path)\n",
    "\n",
    "# # Set the date as the index\n",
    "# df_all.set_index('date', inplace=True)\n",
    "\n",
    "# Check if the index is regularly spaced\n",
    "print(df_all.index.freq)\n",
    "\n",
    "# Make the index regularly spaced (e.g., daily)\n",
    "df_all = df_all.asfreq('D')\n",
    "\n",
    "# Check if the index is regularly spaced\n",
    "print(df_all.index.freq)\n",
    "# Print the first and last dates of the index\n",
    "print(df_all.index[0], df_all.index[-1])\n",
    "\n",
    "print(df_daily.head())\n",
    "\n",
    "# Save to CSV\n",
    "dst = (r'C:\\Users\\daryl\\OneDrive\\Documents\\GDAA3000\\ProjectDischarge\\RdrsSample'\n",
    "       r'\\LstmDatasets\\NsWeatherData\\01ED007')\n",
    "f = '01ED007_weather_from_hourly_2011-2021.csv'\n",
    "df_all.to_csv(os.path.join(dst, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<Day>\n",
      "2011-01-01 00:00:00 2021-12-31 00:00:00\n",
      "Index(['Climate ID', 'max_tem_deg_c', 'min_temp_deg_c', 'mean_temp_deg_c',\n",
      "       'heat_deg_days_deg_c', 'cool_deg_days_deg_c', 'rain_mm', 'snow_cm',\n",
      "       'total_prcp_mm', 'snow_on_grnd_cm', 'dir_max_gust_10s_deg',\n",
      "       'spd_max_gust_km_h', 'dew_point_C', 'rel_hum_percent'],\n",
      "      dtype='object')\n",
      "            Climate ID  max_tem_deg_c  min_temp_deg_c  mean_temp_deg_c  \\\n",
      "date                                                                     \n",
      "2011-01-01     8202000            6.3             0.1              3.2   \n",
      "2011-01-02     8202000            1.9            -1.2              0.4   \n",
      "2011-01-03     8202000            1.6            -4.2             -1.3   \n",
      "2011-01-04     8202000           -1.4            -6.0             -3.7   \n",
      "2011-01-05     8202000           -1.1            -6.0             -3.6   \n",
      "\n",
      "            heat_deg_days_deg_c  cool_deg_days_deg_c  rain_mm  snow_cm  \\\n",
      "date                                                                     \n",
      "2011-01-01                 14.8                  0.0      0.0      0.0   \n",
      "2011-01-02                 17.6                  0.0      1.8      0.0   \n",
      "2011-01-03                 19.3                  0.0      4.4      6.8   \n",
      "2011-01-04                 21.7                  0.0      0.0      0.0   \n",
      "2011-01-05                 21.6                  0.0      0.0      0.0   \n",
      "\n",
      "            total_prcp_mm  snow_on_grnd_cm  dir_max_gust_10s_deg  \\\n",
      "date                                                               \n",
      "2011-01-01            0.0             14.0                   NaN   \n",
      "2011-01-02            1.8              8.0                   NaN   \n",
      "2011-01-03           11.2             10.0                  28.0   \n",
      "2011-01-04            0.0             11.0                  28.0   \n",
      "2011-01-05            0.0             10.0                  26.0   \n",
      "\n",
      "           spd_max_gust_km_h  dew_point_C  rel_hum_percent  \n",
      "date                                                        \n",
      "2011-01-01               <31     2.333333        87.875000  \n",
      "2011-01-02               <31     0.050000        96.166667  \n",
      "2011-01-03                57    -2.150000        88.208333  \n",
      "2011-01-04                54    -8.120833        70.708333  \n",
      "2011-01-05                46    -9.475000        63.000000  \n"
     ]
    }
   ],
   "source": [
    "src = (r'C:\\Users\\daryl\\OneDrive\\Documents\\GDAA3000\\ProjectDischarge\\RdrsSample'\n",
    "       r'\\LstmDatasets\\NsWeatherData\\01ED007')\n",
    "f = '01ED007_weather_2011-2021.csv'\n",
    "\n",
    "df_1 = pd.read_csv(os.path.join(src, f))\n",
    "\n",
    "df_1['date'] = pd.to_datetime(df_1['date'])\n",
    "\n",
    "# # Set the date as the index\n",
    "df_1.set_index('date', inplace=True)\n",
    "\n",
    "# Check if the index is regularly spaced\n",
    "print(df_1.index.freq)\n",
    "\n",
    "# Make the index regularly spaced (e.g., daily)\n",
    "df_1 = df_1.asfreq('D')\n",
    "\n",
    "# Check if the index is regularly spaced\n",
    "print(df_1.index.freq)\n",
    "# Print the first and last dates of the index\n",
    "print(df_1.index[0], df_1.index[-1])\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "df_all = pd.concat([df_1, df_daily], axis=1)\n",
    "\n",
    "# Save to CSV\n",
    "save_path = os.path.join(dst, '01ED007_weather_hourly-daily_joined_2011-2021_daily.csv')\n",
    "df_all.to_csv(save_path)\n",
    "print(df_all.columns)\n",
    "print(df_all.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Compare the 'date' columns of df_1 and df_daily to find non-matching dates\n",
    "# # Sort df_1 by 'date'\n",
    "# df_1 = df_1.sort_values('date')\n",
    "# # Find the dates in df_1 that are not in df_daily\n",
    "# missing_dates = df_1[~df_1['date'].isin(df_daily.index)]\n",
    "# # Count the number of missing dates and print the result\n",
    "# print(len(missing_dates))\n",
    "# print(missing_dates)\n",
    "\n",
    "# # Find the dates in df_daily that are not in df_1\n",
    "# missing_dates_in_daily = df_daily[~df_daily.index.isin(df_1['date'])]\n",
    "# # Count the number of missing dates and print the result\n",
    "# print(len(missing_dates_in_daily))\n",
    "# print(missing_dates_in_daily)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
